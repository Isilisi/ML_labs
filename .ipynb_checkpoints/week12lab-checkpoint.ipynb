{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://bit.ly/2wvWgNy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /Users/ilia/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import treebank\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the raw corpus\n",
    "corpus = treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rudolph', 'NNP'),\n",
       " ('Agnew', 'NNP'),\n",
       " (',', ','),\n",
       " ('55', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('former', 'JJ'),\n",
       " ('chairman', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Consolidated', 'NNP'),\n",
       " ('Gold', 'NNP'),\n",
       " ('Fields', 'NNP'),\n",
       " ('PLC', 'NNP'),\n",
       " (',', ','),\n",
       " ('was', 'VBD'),\n",
       " ('named', 'VBN'),\n",
       " ('*-1', '-NONE-'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('British', 'JJ'),\n",
       " ('industrial', 'JJ'),\n",
       " ('conglomerate', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all the POS-tags and words\n",
    "tag_nos = {}\n",
    "word_nos = {}\n",
    "encoded_corpus = []\n",
    "for sentence in corpus:\n",
    "    encoded_sentence = []\n",
    "    for word, tag in sentence:\n",
    "        encoded_word = word_nos.setdefault(word.lower(), len(word_nos))\n",
    "        encoded_tag = tag_nos.setdefault(tag, len(tag_nos))\n",
    "        encoded_sentence.append( (encoded_word, encoded_tag) )\n",
    "    encoded_corpus.append(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(26, 0),\n",
       " (27, 0),\n",
       " (2, 1),\n",
       " (28, 2),\n",
       " (4, 3),\n",
       " (5, 4),\n",
       " (29, 13),\n",
       " (30, 4),\n",
       " (19, 8),\n",
       " (20, 9),\n",
       " (31, 0),\n",
       " (32, 0),\n",
       " (33, 0),\n",
       " (34, 0),\n",
       " (2, 1),\n",
       " (35, 14),\n",
       " (36, 15),\n",
       " (37, 16),\n",
       " (11, 7),\n",
       " (12, 4),\n",
       " (13, 8),\n",
       " (20, 9),\n",
       " (38, 7),\n",
       " (39, 4),\n",
       " (40, 4),\n",
       " (41, 8),\n",
       " (16, 10)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "train_sentences = encoded_corpus[:-10]\n",
    "test_sentences = encoded_corpus[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the reverse mapping so we can retrieve POS-tags and words using their ids\n",
    "inv_tag_nos = { v:k for k,v in tag_nos.items() }\n",
    "inv_word_nos = { v:k for k,v in word_nos.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE for the initial state probaility vector\n",
    "eps = 0.001\n",
    "Pi = eps * np.ones(len(tag_nos))\n",
    "for sentence in train_sentences:\n",
    "    _, tag = sentence[0]\n",
    "    Pi[tag] += 1\n",
    "Pi /= np.sum(Pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1d6d30b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Pi, bins=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE for transition probabilities matrix\n",
    "A = eps * np.ones((len(tag_nos), len(tag_nos)))\n",
    "for sentence in train_sentences:\n",
    "    prev_tag = None\n",
    "    for word, tag in sentence:\n",
    "        if prev_tag is None:\n",
    "            pass\n",
    "        else:\n",
    "            A[prev_tag, tag] += 1\n",
    "        prev_tag = tag\n",
    "\n",
    "for i in range(len(tag_nos)):\n",
    "    A[i,:] /= np.sum(A[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.transpose(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE for observation probabilities matrix\n",
    "B = eps * np.ones((len(tag_nos), len(word_nos)))\n",
    "for sentence in train_sentences:\n",
    "    for word, tag in sentence:\n",
    "        B[tag, word] += 1\n",
    "\n",
    "for i in range(len(tag_nos)):\n",
    "    B[i,:] /= np.sum(B[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM\n",
    "HMM = (Pi, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(46, 46)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(46, 11387)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for hmm in HMM:\n",
    "    display(hmm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of a sentence using Forward and Backward Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_proba(HMM, sentence):\n",
    "    \n",
    "    # unpack the HMM\n",
    "    Pi, A, B = HMM\n",
    "    \n",
    "    # number of hidden states\n",
    "    N = len(A)\n",
    "    \n",
    "    # length of the sentence\n",
    "    T = len(sentence)\n",
    "    \n",
    "    # initialisation\n",
    "    alpha = np.full(shape = (N, T), fill_value = 0.0)\n",
    "    alpha[:, 0] = Pi * B[:, sentence[0]]\n",
    "    \n",
    "    # induction\n",
    "    for i in range(1, T):\n",
    "        alpha[:, i] = np.matmul(alpha[:, i-1], A) * B[:, sentence[i]]\n",
    "    \n",
    "    # termination\n",
    "    prob = np.sum(alpha[:, T-1])\n",
    "    \n",
    "    return prob\n",
    "\n",
    "def backward_proba(HMM, sentence):\n",
    "    \n",
    "    # unpack the HMM\n",
    "    Pi, A, B = HMM\n",
    "    \n",
    "    # number of hidden states\n",
    "    N = len(A)\n",
    "    \n",
    "    # length of the sentence\n",
    "    T = len(sentence)\n",
    "    \n",
    "    # initialisation\n",
    "    beta = np.full(shape = (N, T), fill_value = 0.0)\n",
    "    beta[:, T-1] = 1.0\n",
    "    \n",
    "    # induction\n",
    "    for i in range(T-2, -1, -1):\n",
    "        beta[:, i] = np.matmul(A, B[:, sentence[i+1]] * beta[:, i+1])\n",
    "    \n",
    "    # termination\n",
    "    prob = np.sum(Pi * beta[:, 0] * B[:, sentence[0]])\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041000999999999996"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.041000999999999996"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example for tutorials\n",
    "Pi0 = np.array([0.3, 0.4, 0.3])\n",
    "\n",
    "A0 = np.array([[0.4, 0.5, 0.1],\n",
    "              [0.1, 0.4, 0.5],\n",
    "              [0.4, 0.5, 0.1]])\n",
    "\n",
    "B0 = np.array([[0.8, 0.1, 0.1],\n",
    "              [0.3, 0.4, 0.3],\n",
    "              [0.1, 0.3, 0.6]])\n",
    "\n",
    "HMM0 = (Pi0, A0, B0)\n",
    "\n",
    "target_sentence = np.array([0, 1, 2])\n",
    "\n",
    "display(forward_proba(HMM0, target_sentence))\n",
    "display(backward_proba(HMM0, target_sentence))\n",
    "# yay! Both algorithms give the same answer as in the tutorials\n",
    "# now find all probabilities of all possible sentences:\n",
    "probs = []\n",
    "for sentence in product([0, 1, 2], [0, 1, 2], [0, 1, 2]):\n",
    "    assert(abs(forward_proba(HMM0, sentence) - backward_proba(HMM0, sentence)) < 0.0001)\n",
    "    probs.append(forward_proba(HMM0, sentence))\n",
    "# by enumerating all possible sentences can find the most likely one, it is [0, 0, 0]\n",
    "# which corresponds to \"brown brown brown\", how come this nonsense sentence is the most likely one?\n",
    "# that's because the numbers were made up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Decoding using Viterbi's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decode(HMM, sentence):\n",
    "    \"\"\"\n",
    "    Implementation of Viterbi's algorithm.\n",
    "    \n",
    "    Input is an HMM, a three-tuple consisting of:\n",
    "    * Pi (the initial probability vector),\n",
    "    * A (the transition probability matrix), and\n",
    "    * B (the observation probability matrix).\n",
    "    \n",
    "    Output should be:\n",
    "    * list of the most likely sequence of hidden states, and\n",
    "    * the maximum delta at the end of the sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    # unpack the HMM\n",
    "    Pi, A, B = HMM\n",
    "    \n",
    "    # number of hidden states\n",
    "    N = len(A)\n",
    "    \n",
    "    # length of the sentence\n",
    "    T = len(sentence)\n",
    "    \n",
    "    # initialize the delta and psi matrices\n",
    "    delta = np.full(shape=(N,T), fill_value=float(\"-inf\"))\n",
    "    psi = np.full(shape=(N,T), fill_value=0)\n",
    "\n",
    "    # (1) Initialize delta and psi for t = 0\n",
    "    delta[:, 0] = Pi * B[:, sentence[0]]\n",
    "    psi[:, 0] = 0\n",
    "    \n",
    "    # (2) Inductive Step\n",
    "    for i in range(1, T):\n",
    "        for j in range(N):\n",
    "            probs = delta[:, i-1] * A[:, j]\n",
    "            psi[j, i] = np.argmax(probs)\n",
    "            delta[j, i] = np.max(probs) * B[j, sentence[i]]\n",
    "            \n",
    "    # (3) Termination/Backtracking\n",
    "    q = np.argmax(delta[:, T-1])\n",
    "    best_prob = delta[q, T-1]\n",
    "    \n",
    "    output = []\n",
    "    for i in range(T-1, -1, -1):\n",
    "        output.append(q)\n",
    "        q = psi[q, i]\n",
    "\n",
    "    return list(reversed(output)), best_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2], 0.0144)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_decode(HMM0, [0, 1, 2]) # same answer as in tutorials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_test_sentence = [word for word, _ in test_sentences[0]]\n",
    "prediction, score = viterbi_decode(HMM, example_test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               TOKEN\t TRUE\t PRED\n",
      "                   a\t   DT\t   DT\n",
      "               white\t  NNP\t  NNP\n",
      "               house\t  NNP\t  NNP\n",
      "           spokesman\t   NN\t   NN\n",
      "                said\t  VBD\t  VBD\n",
      "                last\t   JJ\t   JJ\n",
      "                week\t   NN\t   NN\n",
      "                that\t   IN\t   IN\n",
      "                 the\t   DT\t   DT\n",
      "           president\t   NN\t   NN\n",
      "                  is\t  VBZ\t  VBZ\n",
      "         considering\t  VBG\t  VBG\n",
      "                 *-1\t-NONE-\t-NONE-\n",
      "           declaring\t  VBG\t  VBG\n",
      "                that\t   IN\t   IN\n",
      "                 the\t   DT\t   DT\n",
      "        constitution\t  NNP\t  NNP\n",
      "          implicitly\t   RB\t  NNP\n",
      "               gives\t  VBZ\t  VBZ\n",
      "                 him\t  PRP\t  PRP\n",
      "                 the\t   DT\t   DT\n",
      "           authority\t   NN\t   NN\n",
      "                 for\t   IN\t   IN\n",
      "                   a\t   DT\t   DT\n",
      "           line-item\t   JJ\t   JJ\n",
      "                veto\t   NN\t   NN\n",
      "                 *-2\t-NONE-\t-NONE-\n",
      "                  to\t   TO\t   TO\n",
      "             provoke\t   VB\t   VB\n",
      "                   a\t   DT\t   DT\n",
      "                test\t   NN\t   NN\n",
      "                case\t   NN\t   NN\n",
      "                   .\t    .\t    .\n"
     ]
    }
   ],
   "source": [
    "print('%20s\\t%5s\\t%5s' % ('TOKEN', 'TRUE', 'PRED'))\n",
    "for (wi, ti), pi in zip(test_sentences[0], prediction):\n",
    "    print('%20s\\t%5s\\t%5s' % (inv_word_nos[wi], inv_tag_nos[ti], inv_tag_nos[pi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
