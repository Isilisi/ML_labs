{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School of Computing and Information Systems\n",
    "### The University of Melbourne\n",
    "### COMP30027 MACHINE LEARNING (Semester 1, 2019)\n",
    "### Practical exercises: Week 7\n",
    "Today, we’re going to do a “bake–off” between Logisitic Regression and the classifiers which are\n",
    "its most obvious competitors: Naive Bayes (a simpler probabilistic approach) and Support Vector Machines (using a linear kernel).\n",
    "Don’t forget that you should refer back to earlier weeks or the scikit-learn API where necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. \n",
    "Let’s begin with the simple Iris dataset. Recall that this dataset has four numerical attributes, three classes, and only a small number of instances.\n",
    "Using Zero-R as a baseline, and the following classifiers in their default configurations (i.e. no parameter tuning):\n",
    "\n",
    "(i) naive_bayes.MultinomialNB\n",
    "\n",
    "(ii) naive_bayes.GaussianNB\n",
    "\n",
    "(iii) svm.LinearSVC\n",
    "\n",
    "(iv) svm.SVC (this should the only time you try this model today!)\n",
    "\n",
    "(v) linear_model.LogisticRegression\n",
    "\n",
    "Which has the best accuracy when cross–validating on this dataset? (Note that the whole bake–off should only take at most a couple of seconds to run.) Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-R 0.33333333333333337 time: 0.009168148040771484\n",
      "GNB 0.9533333333333334 time: 0.014063835144042969\n",
      "MNB 0.9533333333333334 time: 0.011994123458862305\n",
      "LinearSVC 0.9666666666666668 time: 0.07668805122375488\n",
      "SVC 0.9800000000000001 time: 0.018596887588500977\n",
      "Logistic Regression 0.9533333333333334 time: 0.015269041061401367\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import numpy as np\n",
    "np.random.seed(30027)\n",
    "iris = datasets.load_iris()\n",
    "#print(dir(iris))\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "models = [DummyClassifier(strategy='most_frequent'),\n",
    "          GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          svm.LinearSVC(),\n",
    "          svm.SVC(),\n",
    "          LogisticRegression()]\n",
    "titles = ['Zero-R',\n",
    "          'GNB',\n",
    "          'MNB',\n",
    "          'LinearSVC',\n",
    "          'SVC',\n",
    "          'Logistic Regression']\n",
    "\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    acc = np.mean(cross_val_score(model, X, y, cv=10))\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is not fully linear so SVC is able to slightly outperform linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "Download the Abalone dataset from the UCI ML repository https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data. The class here is numeric\n",
    "(number of rings, which defines the age of the mollusc), but we can set up a two-class problem:\n",
    "```python\n",
    "def convert_class(raw):\n",
    "    if int(raw)<=10: return 0\n",
    "        else: return 1\n",
    "for line in f:\n",
    "   atts = line[:-1].split(\",\")\n",
    "   X.append(atts[1:-1])\n",
    "   y.append(convert_class(atts[-1]))\n",
    "```\n",
    "\n",
    "Don’t forget to make X as a numpy array using .astype(np.float) — again, using Zero-R as a\n",
    "baseline, and the four classifiers above (forget about SVC() ):\n",
    "\n",
    "### (a)\n",
    "Who wins this bake–off? Why might that be? (It should again take at most a few seconds to\n",
    "run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Python can download files, too (-:\n",
    "#import os\n",
    "#import urllib.request\n",
    "#\n",
    "#if os.path.isfile('abalone.data'):\n",
    "#    print('already downloaded.')\n",
    "#else:\n",
    "#    urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data', 'abalone.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [DummyClassifier(strategy='most_frequent'),\n",
    "          GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          svm.LinearSVC(),\n",
    "          LogisticRegression()]\n",
    "titles = ['Zero-R',\n",
    "          'GNB',\n",
    "          'MNB',\n",
    "          'LinearSVC',\n",
    "          'Logistic Regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-R 0.6535799111906648 time: 0.02172088623046875\n",
      "GNB 0.6767787683728615 time: 0.03435921669006348\n",
      "MNB 0.6535799111906648 time: 0.03149080276489258\n",
      "LinearSVC 0.7744707583215724 time: 0.4357430934906006\n",
      "Logistic Regression 0.7648967907014101 time: 0.06963706016540527\n"
     ]
    }
   ],
   "source": [
    "def convert_class(raw, num_class=2):\n",
    "    raw = int(raw)\n",
    "    if num_class == 2:\n",
    "        if raw<=10: return 0\n",
    "        else: return 1\n",
    "    elif num_class == 3:\n",
    "        if raw <= 8:\n",
    "            return 0\n",
    "        elif 9<=raw<=10:\n",
    "            return 1\n",
    "        elif 11<=raw:\n",
    "            return 2\n",
    "    elif num_class == 29:\n",
    "        return raw\n",
    "\n",
    "def load_abalone(addsex=False, num_class=2):\n",
    "    X, y = [], []\n",
    "    with open('abalone.data', 'r') as fin:\n",
    "        for line in fin:\n",
    "            atts = line[:-1].split(\",\")\n",
    "            if not addsex:\n",
    "                X.append(atts[1:-1])\n",
    "            else:\n",
    "                sex = atts[0]\n",
    "                if sex == \"M\": sex = 0\n",
    "                elif sex==\"I\": sex = 1\n",
    "                elif sex==\"F\": sex = 2\n",
    "                else: sex = 3\n",
    "                \n",
    "                X.append([sex] + atts[1:-1])\n",
    "            y.append(convert_class(atts[-1], num_class))\n",
    "    X = np.array(X, dtype=float)\n",
    "    return X, y\n",
    "\n",
    "X, y = load_abalone(addsex=False, num_class=2)\n",
    "\n",
    "#print(X[0], y[0])\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    acc = np.mean(cross_val_score(model, X, y, cv=10))\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB here is largely the same as 0-R: this is to be expected for the Multinomial NB (as the attributes are not frequencies); it's somewhat surprising that the Gaussian NB is almost as bad - perhaps the distributions are not normal because the data has outliers.\n",
    "\n",
    "LinearSVC is very slightly better than Logistic Regression here, but the difference is small, and might not be significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### (b) \n",
    "Modify convert_class() so that it instead sets up a three-class problem: 8 rings or less; 9\n",
    "or 10 rings; 11 rings or more — which classifier wins now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-R 0.34642075045918785 time: 0.018862009048461914\n",
      "GNB 0.577260492694719 time: 0.03551197052001953\n",
      "MNB 0.5570936039372009 time: 0.03243899345397949\n",
      "LinearSVC 0.644483665671723 time: 1.0409269332885742\n",
      "Logistic Regression 0.6320370619049334 time: 0.1367959976196289\n"
     ]
    }
   ],
   "source": [
    "X, y = load_abalone(addsex=False, num_class=3)\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    acc = np.mean(cross_val_score(model, X, y, cv=10))\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly this is the same story, but notice that NB now is substantially better than 0-R. It seems that there are some patterns that NB can find after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) \n",
    "Set up the 29-class problem (!) as follows: y.append(int(atts[-1])) — now which classifier is the best?\n",
    "\n",
    "Naive Bayes seems like it would be woefully inadequate for a problem like\n",
    "this: how is it going? This run will probably take noticeably longer than the others — which classifier is the main culprit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nj/ENV3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/nj/ENV3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/nj/ENV3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/nj/ENV3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-R 0.16500855703117906 time: 0.03310108184814453\n",
      "GNB 0.23591700319832828 time: 0.08421778678894043\n",
      "MNB 0.16525547061142598 time: 0.05653882026672363\n",
      "LinearSVC 0.258368737839951 time: 4.7963268756866455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nj/ENV3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 0.24687601563564093 time: 1.2486748695373535\n"
     ]
    }
   ],
   "source": [
    "X, y = load_abalone(addsex=False, num_class=29)\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    acc = np.mean(cross_val_score(model, X, y, cv=10))\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things we can notice here; one is that scikit-learn is not impressed that we are trying to cross-validate with so few instances of each class. This will guarantee that stratification is impossible, which is undesirable in an evaluation framework.\n",
    "\n",
    "Looking at the performance, the 29-class task is far more difficult; even the better models are not much better than 0-R. Multinomial NB is back to useless, but now Gaussian NB is roughly as effective as the more complex linear models. One reason for this is probably that there is very little information available to support a prediction.\n",
    "\n",
    "The LinearSVC is now noticeably slower than the other models; this is to be expected with a large number of classes. SVC would be very difficult to train in this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### (d) \n",
    "The gender attribute ( atts[0] ) is mostly un-helpful for this problem. Nevertheless, let’s\n",
    "incorporate it into the models as a one–hot attribute and see what happens:\n",
    "Pay particularly close attention to the performance on Abalone-29: which model(s) are most\n",
    "resilient to the addition of these three attributes? Why do you suppose this would be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before (4177, 8)\n",
      "[[1.     0.     0.     0.455  0.365  0.095  0.514  0.2245 0.101  0.15  ]\n",
      " [1.     0.     0.     0.35   0.265  0.09   0.2255 0.0995 0.0485 0.07  ]\n",
      " [0.     0.     1.     0.53   0.42   0.135  0.677  0.2565 0.1415 0.21  ]\n",
      " [1.     0.     0.     0.44   0.365  0.125  0.516  0.2155 0.114  0.155 ]\n",
      " [0.     1.     0.     0.33   0.255  0.08   0.205  0.0895 0.0395 0.055 ]]\n",
      "after (4177, 10)\n",
      "Zero-R 0.16500855703117906 time: 0.029197216033935547\n",
      "GNB 0.09995373301345349 time: 0.09067392349243164\n",
      "MNB 0.2200867294398984 time: 0.05632901191711426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nj/ENV3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/nj/ENV3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/nj/ENV3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/nj/ENV3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC 0.24991067442390813 time: 6.016511917114258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nj/ENV3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 0.24658107691038467 time: 1.3327722549438477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X, y = load_abalone(addsex=True, num_class=29)\n",
    "print('before', X.shape)\n",
    "ohe = OneHotEncoder(categorical_features=[0])\n",
    "X = ohe.fit_transform(X).toarray()\n",
    "print(X[0:5])\n",
    "print('after', X.shape)\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    acc = np.mean(cross_val_score(model, X, y, cv=10))\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear models are essentially the same - it's likely that the weights for the extra attributes are close to 0. Multinomial NB is surprisingly _better_ in this situation, but still pretty terrible. Gaussian NB has totally fallen apart, as the one-hot distribution is decidedly not normal, so the probabilities that are being \"learned\" for the gender attribute are actually dominating the more meaningful probabilities for the other attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Let’s look at something a little bigger: the US census data Adult. We’ve uploaded a slightly pre-processed version of this dataset (adult.txt) to the LMS.\n",
    "Note that this dataset is substantially larger; we recommend saving time by doing hold–out instead of cross–validation — but note that the accuracies you observe will be more subject to variance.\n",
    "Many of the attributes are nominal; for now, let’s ignore them (or you can convert to one-hot if you’re feeling brave!). An inelegant way of loading the data is to smush the numeric attributes into a list, for example:\n",
    "```python\n",
    "X.append([atts[0],atts[2],atts[4],atts[10],atts[11],atts[12]])\n",
    "```\n",
    "Don’t forget to convert the class to integers! Which of the classifiers wins this bake–off? What might be different about this problem, compared to the previous ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-R 0.7632968922736765 time: 0.005110025405883789\n",
      "GNB 0.8008844122343692 time: 0.012317895889282227\n",
      "MNB 0.7897064242722024 time: 0.010676145553588867\n",
      "LinearSVC 0.3000859845227859 time: 1.7251451015472412\n",
      "Logistic Regression 0.8059206485689718 time: 0.06301021575927734\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "with open('adult.txt', mode='r') as fin:\n",
    "    for line in fin:\n",
    "        atts = line.strip().split(\",\")\n",
    "        X.append([atts[0],atts[2],atts[4],atts[10],atts[11],atts[12]])\n",
    "        #labels: >50K, <=50K.\n",
    "        if atts[-1] == '>50K':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        y.append(label)\n",
    "\n",
    "X = np.array(X, dtype=float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    model.fit(X_train,y_train)\n",
    "    acc = accuracy_score(model.predict(X_test),y_test)\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't much to choose between these methods; they are all slightly better than 0-R, but only barely.\n",
    "\n",
    "If you run this a few times, you will find that suddenly LinearSVC will become horrible (~30% accuracy). For some random partitions, the SVM will learn entirely the wrong margin (effectively, over-fitting the training data). This is significant if we're using repeated random subsampling or cross-validation, as we will see that the performance is worse than the other models, but we won't necessarily realise why. Most of the folds will be just as good as the other models, but one fold will randomly be far worse, lowering the \"average\" effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. \n",
    "A better way of loading in a mixed data set like this one is the DictVectorizer() , which converts an array of dictionaries into a sparse representation , for example:\n",
    "```python\n",
    "for line in f:\n",
    "    atts = line[:-1].split(\",\")\n",
    "    this = {}\n",
    "    this[\"age\"]=int(atts[0])\n",
    "    this[\"workclass\"]=atts[1]\n",
    "    ...\n",
    "    X.append(this)\n",
    "```\n",
    "(Note that the numeric attributes shouldn’t be left as strings!) and:\n",
    "```python\n",
    "vec = DictVectorizer()\n",
    "X = vec.fit_transform(X).toarray()\n",
    "```\n",
    "\n",
    "[For completeness, here are the attribute specifications from UCI.]\n",
    "age: continuous.\n",
    "\n",
    "workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "\n",
    "fnlwgt: continuous.\n",
    "\n",
    "education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "\n",
    "education-num: continuous.\n",
    "\n",
    "marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "\n",
    "occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "\n",
    "relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "\n",
    "race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "\n",
    "sex: Female, Male.\n",
    "\n",
    "capital-gain: continuous.\n",
    "\n",
    "capital-loss: continuous.\n",
    "\n",
    "hours-per-week: continuous.\n",
    "\n",
    "native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\n",
    "\n",
    "\n",
    "### (a) \n",
    "Any changes to the winner of the bake–off? Was it worthwhile adding all of these various\n",
    "extra attributes? (There are a lot!)\n",
    "### (b) \n",
    "As a final summary, compare the results of the four classifiers (and the baseline) when cross–validating over this large dataset. (It will probably take a few minutes to run.) Confirm that the averaged accuracy for LinearSVC is consistent with what you expect, based on your observations across the folds (or when trying different hold–out partitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 21755)\n",
      "Zero-R 0.7591904454179904 time: 53.013346672058105\n",
      "GNB 0.8008352836945651 time: 441.9942111968994\n",
      "MNB 0.7791530009344381 time: 93.86210513114929\n",
      "LinearSVC 0.7951229085959625 time: 102.93285512924194\n",
      "Logistic Regression 0.8508953601019469 time: 81.79078817367554\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "X, y = [], []\n",
    "with open('adult.txt', mode='r') as fin:\n",
    "    for line in fin:\n",
    "        atts = line.strip().split(\",\")\n",
    "        if len(atts) != 15:\n",
    "            continue\n",
    "        atts = [att.strip() for att in atts]\n",
    "        this = {}\n",
    "        this['age'] = int(atts[0])\n",
    "        this['workclass'] = atts[1]\n",
    "        this['fnlwgt'] = atts[2]\n",
    "        this['education'] = atts[3]\n",
    "        this['education-num'] = int(atts[4])\n",
    "        this['marital-status'] = atts[5]\n",
    "        this['occupation'] = atts[6]\n",
    "        this['relationship'] = atts[7]\n",
    "        this['race'] = atts[8]\n",
    "        this['sex'] = atts[9]\n",
    "        this['capital-gain'] = int(atts[10])\n",
    "        this['capital-loss'] = int(atts[11])\n",
    "        this['hours-per-week'] = int(atts[12])\n",
    "        this['native-country'] = atts[13]\n",
    "\n",
    "        X.append(this)\n",
    "        #labels: >50K, <=50K.\n",
    "        if atts[-1] == '>50K':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        y.append(label)\n",
    "\n",
    "vec = DictVectorizer()\n",
    "X = vec.fit_transform(X).toarray()\n",
    "print(X.shape)\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    start = time.time()\n",
    "    acc = np.mean(cross_val_score(model, X, y, cv=5))\n",
    "    end = time.time()\n",
    "    t = end - start\n",
    "    print(title, acc, 'time:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to just the numeric features, these one-hot nominal features don't help NB very much. On the other hand, both Logistic Regression and LinearSVC are noticeably improved, so there appear to be some predictive attribute values.\n",
    "\n",
    "LinearSVC should actually have roughly the same performance as Logistic Regression here; again, it's likely that one of the x-val partitions has a margin that is overfitting, and is substantially lowering the overall accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
