{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Practical, Week 4 - sample solutions\n",
    "\n",
    "Today, we will be using scikit-learn to classify some data, and to evaluate some classifiers.\n",
    "# 1. \n",
    "Load the Iris dataset 1 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "iris = datasets.load_iris()\n",
    "#print(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a)\n",
    "Identify the contents of the complex data type iris , for example iris.DESCR contains a long\n",
    "description of the dataset, which you can print() .\n",
    "\n",
    "\n",
    "### 1b) \n",
    "The common terminology in scikit-learn is that the array defining the attribute values is\n",
    "called X and the array defining the gold–standard (“ground truth”) labels is called y ; create\n",
    "these variables for the Iris data.\n",
    "\n",
    "\n",
    "### 1(c) \n",
    "Confirm that X is a 2-dimensional array, with a row for each instance and a column for each\n",
    "attribute. (Hint: read about the shape property in numpy .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'feature_names', 'target', 'target_names']\n",
      "X.shape (150, 4) y.shape (150,)\n"
     ]
    }
   ],
   "source": [
    "print(dir(iris))\n",
    "#print(iris.DESCR)\n",
    "#print(iris.data.shape)\n",
    "#print(iris.target.shape)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(\"X.shape {} y.shape {}\".format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\n",
    "Let’s build a 0-R classifier (“majority class classifier”). In scikit-learn , this is a DummyClassifier.\n",
    "\n",
    "Note that scikit-learn uses this terminology to help remind you not to use these sorts of classifiers when trying to solve real\n",
    "problems; they are easy baseline classifiers, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(constant=None, random_state=None, strategy='most_frequent')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "zero_r = DummyClassifier(strategy='most_frequent')\n",
    "zero_r.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(a) \n",
    "Confirm that this is a typical 0-R classifier by checking its predictions on the training data:\n",
    "zero_r.predict(X) — which class has it chosen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 50), (1, 50), (2, 50)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ybar = zero_r.predict(X)\n",
    "print(ybar)\n",
    "label_counter = Counter(y)\n",
    "label_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is a number, as far as scikit-learn is concerned - so it's a little difficult to know which class label this is. On the other hand, each of the classes is equally likely, so the method appears to have chosen the \"0th\" class arbitrarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b) \n",
    "The default evaluation metric associated with a DummyClassifier is accuracy, which you\n",
    "can observe using score() , for example: zero_r.score(X, y)\n",
    "This strategy — building a model, and then evaluating on the data that we used to build the\n",
    "model — gives us something called “training accuracy”, and is generally frowned upon in\n",
    "the Machine Learning community. Why do you suppose this is? (We’ll examine some better\n",
    "techniques later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(zero_r.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(c) \n",
    "Contrast the 0-R classifier with the “weighted random classifier”, which makes random pre-\n",
    "dictions according to the distribution of classes in the training data; (strategy='stratified')\n",
    "— check its predictions, and evaluate its training accuracy. Does it have a higher accuracy,\n",
    "on average, than 0-R, or a lower accuracy? (You should run score() at least 10 times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2866666666666667, 0.34, 0.34, 0.29333333333333333, 0.26, 0.36666666666666664, 0.41333333333333333, 0.32, 0.35333333333333333, 0.35333333333333333]\n",
      "Average accuracy over 10 runs is: 0.3326666666666667.\n"
     ]
    }
   ],
   "source": [
    "stratified_clf = DummyClassifier(strategy='stratified')\n",
    "stratified_clf.fit(X, y)\n",
    "accuracies = []\n",
    "num_runs = 10\n",
    "for i in range(num_runs):\n",
    "    acc = stratified_clf.score(X, y)\n",
    "    accuracies.append(acc)\n",
    "print(accuracies)\n",
    "print('Average accuracy over {} runs is: {}.'.format(num_runs, np.mean(accuracies)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 runs isn't quite enough to be certain which of these methods has greater accuracy. In fact, they will generally produce the same accuracy for class distributions where all of the classes are equally likely - but it would require more than 10 iterations to really see this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\n",
    "Let’s consider a couple of other classifiers: a Decision Tree, and 1-R 4 (which is really just a limited\n",
    "DecisionTreeClassifier in scikit-learn ).\n",
    "\n",
    "Note that scikit-learn implementation of 1-R is slightly different to the lecture version, because it doesn’t count errors —\n",
    "rather it uses the Gini coefficient or the Information Gain to determine the best attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "one_r = DecisionTreeClassifier(max_depth=1)\n",
    "one_r.fit(X, y)\n",
    "dt = DecisionTreeClassifier(max_depth=None)\n",
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(a) \n",
    "Find the training accuracy of the two classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-R accuracy: 0.6666666666666666; DT accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "one_r_acc = one_r.score(X, y)\n",
    "dt_acc = dt.score(X, y)\n",
    "print(\"1-R accuracy: {}; DT accuracy: {}\".format(one_r_acc, dt_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the Decision Tree makes fewer errors when testing on the training data. The fact that the DT accuracy is 100% shows that every leaf has a homogeneous class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(b) \n",
    "The \"feature importances\" attribute is adequate for completely describing the 1-R classi-\n",
    "fier. Which attribute is being used to classify the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petal length (cm)\n"
     ]
    }
   ],
   "source": [
    "importances = one_r.feature_importances_\n",
    "max_index = np.argmax(importances)\n",
    "best_feature_name = iris.feature_names[max_index]\n",
    "print(best_feature_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c) \n",
    "(Harder) Check the predicted labels for each instance to discern the values for this attribute\n",
    "that each class maps to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMAX9gmsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109401908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ybar = one_r.predict(X)\n",
    "best_feature = X[:, max_index]\n",
    "plt.scatter(best_feature, ybar, c=ybar)\n",
    "plt.xlabel(best_feature_name)\n",
    "plt.ylabel('predicted class')\n",
    "plt.show()\n",
    "#print(ybar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the scatter-plot above isn't the ideal way to visualise this data, but it does allow us to readily observe that small values of petal-width are labelled as class 0, and larger values as class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(d) \n",
    "The default splitting criterion for these Decision Trees is the Gini coefficient. Read up on\n",
    "the difference between this and the Information Gain — do you expect the behaviour of\n",
    "this model to change by using the alternative splitting criterion? Try it, and confirm your\n",
    "expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain/entropy: 1-R accuracy: 0.6666666666666666 DT accuracy: 1.0\n",
      "1-R attribute:  petal width (cm)\n"
     ]
    }
   ],
   "source": [
    "one_r = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")\n",
    "one_r.fit(X, y)\n",
    "dt = DecisionTreeClassifier(max_depth=None, criterion=\"entropy\")\n",
    "dt.fit(X, y)\n",
    "\n",
    "one_r_acc = one_r.score(X, y)\n",
    "dt_acc = dt.score(X, y)\n",
    "print(\"Information Gain/entropy: 1-R accuracy: {} DT accuracy: {}\".format(one_r_acc, dt_acc))\n",
    "\n",
    "importances = one_r.feature_importances_\n",
    "max_index = np.argmax(importances)\n",
    "best_feature_name = iris.feature_names[max_index]\n",
    "print(\"1-R attribute: \",best_feature_name)\n",
    "ybar = one_r.predict(X)\n",
    "best_feature = X[:, max_index]\n",
    "#plt.scatter(best_feature, ybar, c=ybar)\n",
    "#plt.xlabel(best_feature_name)\n",
    "#plt.ylabel('predicted class')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that entropy is $H(X) = -\\sum p(x) \\log p(x)$; Gini is defined similarly: $G(X) = 1 - \\sum p(x)\\times p(x)$\n",
    "\n",
    "As we might expect from the similar formulae, whether we use the Gini coefficient or Information Gain makes little difference in this case: 1-R still chooses petal-width, and the Decision Tree still reproduces the training data exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. \n",
    "A better mechanism for evaluating a classifier is based on randomly partitioning the data into a\n",
    "training set and test set (the “holdout” method). There is an in-built utility for this in scikit-learn ,\n",
    "but it can be in one of two places:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (112, 4) X_test: (38, 4) y_train: (112,) y_test: (38,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # Newer versions\n",
    "#from sklearn.cross_validation import train_test_split # Older versions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print('X_train: {} X_test: {} y_train: {} y_test: {}'.format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(a)\n",
    "Train the three classifiers (0-R, 1-R, Decision Tree) on the training data, rather than the full\n",
    "data set. score() is too specific to be used in most situations; another way to find the training\n",
    "accuracy is by comparing the predictions to the gold-standard labels as follows:\n",
    "```python\n",
    ">>> from scikit-learn.metrics import accuracy_score\n",
    ">>> accuracy_score(zero_R.predict(X_train),y_train))\n",
    "```\n",
    "\n",
    "Calculate the accuracy of the classifiers on the held-out training data. How does it\n",
    "compare to the training accuracies you calculated before? Why is this?\n",
    "\n",
    "### 4(b) \n",
    "Instead of calculating the accuracy with respect to the training set, train your classifiers on\n",
    "the training data (using fit() ) and then evaluate them (by calculating accuracy) accord-\n",
    "ing to their predictions on the test data. How different are the training accuracies and test\n",
    "accuracies? Hypothesise what could be causing these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracies: 0-R: 0.3392857142857143 1-R: 0.6607142857142857 DT: 1.0\n",
      "Test accuracies: 0-R: 0.3157894736842105 1-R: 0.6842105263157895 DT: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "zero_r.fit(X_train, y_train)\n",
    "one_r.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "zr_acc = accuracy_score(zero_r.predict(X_train),y_train)\n",
    "or_acc = accuracy_score(one_r.predict(X_train),y_train)\n",
    "dt_acc = accuracy_score(dt.predict(X_train),y_train)\n",
    "print('Train accuracies: 0-R: {} 1-R: {} DT: {}'.format(zr_acc, or_acc, dt_acc))\n",
    "\n",
    "zr_acc = accuracy_score(zero_r.predict(X_test),y_test)\n",
    "or_acc = accuracy_score(one_r.predict(X_test),y_test)\n",
    "dt_acc = accuracy_score(dt.predict(X_test),y_test)\n",
    "print('Test accuracies: 0-R: {} 1-R: {} DT: {}'.format(zr_acc, or_acc, dt_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test data, which we didn't use to build our models, is - perhaps unsurprisingly - somewhat worse than the accuracy on the training data.\n",
    "\n",
    "In the case of the Decision Tree, it is worthwhile to observe that the tree is not actually making perfect predictions, although on the whole, it is still pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(c) \n",
    "By default, train_test_split uses 75% of the data as training, and 25% as test. This can\n",
    "be changed by passing an argument, for example, test_size=0.5 means that we use 50%\n",
    "as training 5 and 50% as test. Try some different values (perhaps multiple times) to see if you\n",
    "can observe the trade-off inherent in the model using this evaluation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments with test set size: 0.01\n",
      "X_train: (148, 4) X_test: (2, 4) y_train: (148,) y_test: (2,)\n",
      "Train accuracies: 0-R: 0.33783783783783783 1-R: 0.668918918918919 DT: 1.0\n",
      "Test accuracies: 0-R: 0.0 1-R: 0.5 DT: 1.0\n",
      "\n",
      "Running experiments with test set size: 0.1\n",
      "X_train: (135, 4) X_test: (15, 4) y_train: (135,) y_test: (15,)\n",
      "Train accuracies: 0-R: 0.35555555555555557 1-R: 0.7037037037037037 DT: 1.0\n",
      "Test accuracies: 0-R: 0.13333333333333333 1-R: 0.3333333333333333 DT: 0.9333333333333333\n",
      "\n",
      "Running experiments with test set size: 0.2\n",
      "X_train: (120, 4) X_test: (30, 4) y_train: (120,) y_test: (30,)\n",
      "Train accuracies: 0-R: 0.35833333333333334 1-R: 0.6666666666666666 DT: 1.0\n",
      "Test accuracies: 0-R: 0.23333333333333334 1-R: 0.6666666666666666 DT: 0.9666666666666667\n",
      "\n",
      "Running experiments with test set size: 0.3\n",
      "X_train: (105, 4) X_test: (45, 4) y_train: (105,) y_test: (45,)\n",
      "Train accuracies: 0-R: 0.3523809523809524 1-R: 0.6952380952380952 DT: 1.0\n",
      "Test accuracies: 0-R: 0.28888888888888886 1-R: 0.6 DT: 0.9333333333333333\n",
      "\n",
      "Running experiments with test set size: 0.4\n",
      "X_train: (90, 4) X_test: (60, 4) y_train: (90,) y_test: (60,)\n",
      "Train accuracies: 0-R: 0.36666666666666664 1-R: 0.6444444444444445 DT: 1.0\n",
      "Test accuracies: 0-R: 0.2833333333333333 1-R: 0.7 DT: 0.9833333333333333\n",
      "\n",
      "Running experiments with test set size: 0.5\n",
      "X_train: (75, 4) X_test: (75, 4) y_train: (75,) y_test: (75,)\n",
      "Train accuracies: 0-R: 0.3466666666666667 1-R: 0.6666666666666666 DT: 1.0\n",
      "Test accuracies: 0-R: 0.32 1-R: 0.6666666666666666 DT: 0.9333333333333333\n",
      "\n",
      "Running experiments with test set size: 0.6\n",
      "X_train: (60, 4) X_test: (90, 4) y_train: (60,) y_test: (90,)\n",
      "Train accuracies: 0-R: 0.38333333333333336 1-R: 0.7333333333333333 DT: 1.0\n",
      "Test accuracies: 0-R: 0.3 1-R: 0.6222222222222222 DT: 0.9666666666666667\n",
      "\n",
      "Running experiments with test set size: 0.7\n",
      "X_train: (45, 4) X_test: (105, 4) y_train: (45,) y_test: (105,)\n",
      "Train accuracies: 0-R: 0.35555555555555557 1-R: 0.6888888888888889 DT: 1.0\n",
      "Test accuracies: 0-R: 0.3238095238095238 1-R: 0.6571428571428571 DT: 0.9523809523809523\n",
      "\n",
      "Running experiments with test set size: 0.8\n",
      "X_train: (30, 4) X_test: (120, 4) y_train: (30,) y_test: (120,)\n",
      "Train accuracies: 0-R: 0.36666666666666664 1-R: 0.7333333333333333 DT: 1.0\n",
      "Test accuracies: 0-R: 0.325 1-R: 0.5833333333333334 DT: 0.9333333333333333\n",
      "\n",
      "Running experiments with test set size: 0.9\n",
      "X_train: (15, 4) X_test: (135, 4) y_train: (15,) y_test: (135,)\n",
      "Train accuracies: 0-R: 0.3333333333333333 1-R: 0.6666666666666666 DT: 1.0\n",
      "Test accuracies: 0-R: 0.3333333333333333 1-R: 0.6666666666666666 DT: 0.8814814814814815\n",
      "\n",
      "Running experiments with test set size: 0.99\n",
      "X_train: (1, 4) X_test: (149, 4) y_train: (1,) y_test: (149,)\n",
      "Train accuracies: 0-R: 1.0 1-R: 1.0 DT: 1.0\n",
      "Test accuracies: 0-R: 0.3288590604026846 1-R: 0.3288590604026846 DT: 0.3288590604026846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test_size in [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]:\n",
    "    print('Running experiments with test set size: {}'.format(test_size))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    print('X_train: {} X_test: {} y_train: {} y_test: {}'.format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))\n",
    "    \n",
    "    zero_r.fit(X_train, y_train)\n",
    "    one_r.fit(X_train, y_train)\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    zr_acc = accuracy_score(zero_r.predict(X_train),y_train)\n",
    "    or_acc = accuracy_score(one_r.predict(X_train),y_train)\n",
    "    dt_acc = accuracy_score(dt.predict(X_train),y_train)\n",
    "    print('Train accuracies: 0-R: {} 1-R: {} DT: {}'.format(zr_acc, or_acc, dt_acc))\n",
    "\n",
    "    zr_acc = accuracy_score(zero_r.predict(X_test),y_test)\n",
    "    or_acc = accuracy_score(one_r.predict(X_test),y_test)\n",
    "    dt_acc = accuracy_score(dt.predict(X_test),y_test)\n",
    "    print('Test accuracies: 0-R: {} 1-R: {} DT: {}'.format(zr_acc, or_acc, dt_acc))\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to run this more than once to see how drastically the performance can change on the smaller test sets. For the larger test sets, the performance is rather consistent (in the sense that it is usually bad), but there isn't quite enough training data to build an accurate model.\n",
    "\n",
    "We might also note that the training accuracy is almost better than the test accuracy for these classifiers; this is yet another reason to be suspicious of training accuracy (as it tends to over-estimate the true performance of our classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. \n",
    "(Stratified) M –fold cross-validation is so popular, scikit-learn has a utility that applies it di-\n",
    "rectly 6 . For example, 10–fold cross-validation of the 0-R classifier proceeds as follows:\n",
    "```python\n",
    ">>> from sklearn.model_validation import cross_val_score # Newer versions\n",
    ">>> from sklearn.cross_validation import cross_val_score # Older versions\n",
    ">>> cross_val_score(zero_R, X, y, cv=10)\n",
    "```\n",
    "\n",
    "### 5(a) \n",
    "This method returns an array of the calculated evaluation metric (by default, accuracy) across\n",
    "the folds. Write a wrapper function which averages these values, so as to come up with a\n",
    "single score for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(zero_r, X, y, cv=10))\n",
    "\n",
    "def avg_score(clf, X, y, cv=10):\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(b) \n",
    "How does the estimate of the accuracy of the various classifiers using cross-validation compare to the training accuracies and holdout accuracies you calculated above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier(constant=None, random_state=None, strategy='most_frequent')\n",
      "Average CV accuracy 0.33333333333333337\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Average CV accuracy 0.6666666666666667\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Average CV accuracy 0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf in [zero_r, one_r, dt]:\n",
    "    avg = avg_score(clf, X, y, cv=10)\n",
    "    print(clf)\n",
    "    print('Average CV accuracy', avg)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, for our baseline classifiers, the CV accuracy is very much like the training accuracy - this occurs because of stratification. Arguably, holdout can be a little unfair for these methods because the class distribution can be different between the training and test sets, simply due to random chance.\n",
    "\n",
    "As for the Decision Tree, the CV accuracy looks fairly similar to the test accuracy for many of the holdout split sizes. The main advantage is that this value doesn't tend to change quite as much as holdout test accuracy, if we repeat the procedure. A secondary advantage is that we don't need to try to justify why some particular holdout split size is appropriate - stratified 10-fold CV is just a \"standard\" strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
