{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School of Computing and Information Systems\n",
    "### The University of Melbourne\n",
    "### COMP30027 M ACHINE L EARNING (Semester 1, 2019)\n",
    "### Practical exercises: Week 12\n",
    "\n",
    "Let’s re-visit the three-class (8 or fewer, 9 or 10, 11 or more rings) Abalone task, but this time in an unsupervised context.\n",
    "\n",
    "# 1. \n",
    "Use the method of k-means to cluster the Abalone-3 data. (It’s available from sklearn.cluster.KMeans )\n",
    "### (a) \n",
    "Confirm that you understand the difference between the fit() and predict() methods in\n",
    "this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (4177, 7) y: {0, 1, 2}\n",
      "cluster centers: [[0.40149533 0.30629907 0.10226791 0.34381184 0.14777445 0.07480249\n",
      "  0.10484081]\n",
      " [0.56974698 0.44602933 0.1523433  0.92726337 0.39996435 0.20276509\n",
      "  0.26923433]\n",
      " [0.6644958  0.52396759 0.1845078  1.55741537 0.68230732 0.33814286\n",
      "  0.43352761]]\n",
      "assigned clusters: (4177,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def convert_class(raw, num_class=2):\n",
    "    raw = int(raw)\n",
    "    if num_class == 2:\n",
    "        if raw<=10: return 0\n",
    "        else: return 1\n",
    "    elif num_class == 3:\n",
    "        if raw <= 8:\n",
    "            return 0\n",
    "        elif 9<=raw<=10:\n",
    "            return 1\n",
    "        elif 11<=raw:\n",
    "            return 2\n",
    "    elif num_class == 29:\n",
    "        return raw\n",
    "\n",
    "def load_abalone(addsex=False, num_class=2):\n",
    "    X, y = [], []\n",
    "    with open('abalone.data', 'r') as fin:\n",
    "        for line in fin:\n",
    "            atts = line[:-1].split(\",\")\n",
    "            if not addsex:\n",
    "                X.append(atts[1:-1])\n",
    "            else:\n",
    "                sex = atts[0]\n",
    "                if sex == \"M\": sex = 0\n",
    "                elif sex==\"I\": sex = 1\n",
    "                elif sex==\"F\": sex = 2\n",
    "                else: sex = 3\n",
    "                \n",
    "                X.append([sex] + atts[1:-1])\n",
    "            y.append(convert_class(atts[-1], num_class))\n",
    "    X = np.array(X, dtype=float)\n",
    "    return X, y\n",
    "\n",
    "X, y = load_abalone(addsex=False, num_class=3)\n",
    "print('X:', X.shape, 'y:', set(y))\n",
    "\n",
    "num_clusters = 3\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(X)\n",
    "centers = km.cluster_centers_\n",
    "print('cluster centers:', centers)\n",
    "\n",
    "clusters = km.predict(X)\n",
    "print('assigned clusters:', clusters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) \n",
    "Write a function that calculates the entropy and purity of a k-means cluster, based on the\n",
    "true labels of Abalone-3. \n",
    "\n",
    "$\\textit{purity} = \\sum_{i=1}^K \\frac{|C_i|}{N}\\max_j P_i(j)$\n",
    "\n",
    "\n",
    "$\\textit{entropy} = \\sum_{i=1}^K \\frac{|C_i|}{N} H(x_i)$\n",
    "    \n",
    "where $x_i$ is the distribution of actual class labels in cluster\n",
    "    $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster purity is 0.5633229590615274 and entropy is 0.9052236945735381\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy as scipyentropy\n",
    "from collections import Counter\n",
    "\n",
    "y = np.array(y, dtype=int)\n",
    "\n",
    "\n",
    "def eval_cluster(clusters, y, num_clusters):\n",
    "    purity = 0\n",
    "    entropy = 0\n",
    "    num_instances = clusters.shape[0]\n",
    "    for i in range(num_clusters):\n",
    "        C_i = np.sum(clusters==i)\n",
    "        #print('ys in cluster {}: {}'.format(i, y[clusters==i]))\n",
    "        unique, counts = np.unique(y[clusters==i], return_counts=True)\n",
    "        #print(unique, counts)\n",
    "        assert np.sum(counts) == C_i\n",
    "        P_i = counts/C_i\n",
    "        P_imaxj = np.max(P_i)\n",
    "        purity += P_imaxj * C_i / num_instances\n",
    "        H_i = scipyentropy(P_i)\n",
    "        entropy += H_i * C_i / num_instances\n",
    "\n",
    "    return purity, entropy\n",
    "\n",
    "\n",
    "purity, entropy = eval_cluster(clusters, y, num_clusters)\n",
    "print('cluster purity is {} and entropy is {}'.format(purity, entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) \n",
    "Calculate the unsupervised evaluation metric known as Sum of Squared Errors on the re-\n",
    "sulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 84.41410489  84.2124651  102.74326353] 271.3698335253488 271.3698335253488\n"
     ]
    }
   ],
   "source": [
    "def sse_cluster(X, clusters, centers):\n",
    "    sses = np.zeros(centers.shape[0])\n",
    "    for i in range(centers.shape[0]):\n",
    "        sses[i] = np.sum((X[clusters==i] - centers[i]) ** 2, dtype=np.float64)\n",
    "    return sses\n",
    "#one sse for each cluster\n",
    "sses = sse_cluster(X, clusters, centers)\n",
    "#we didn't have to compute SSE, km.inertia was already there, but good practice!\n",
    "print(sses, np.sum(sses), km.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) \n",
    "k-means is typically re-run multiple times. In this case, it is controlled by the n_init parameter (which defaults to 10). Re-evaluate the resulting clusters for different values of n_init\n",
    "(especially 1 or 2, but perhaps also some larger values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_init: 1 sse:271.3755366704529 purity:0.5647593966961935 entropy:0.9046057816221923\n",
      "n_init: 2 sse:271.37354882977144 purity:0.5649988029686378 entropy:0.9046979431715132\n",
      "n_init: 3 sse:271.37354882977144 purity:0.5649988029686378 entropy:0.9046979431715131\n",
      "n_init: 10 sse:271.37354882977144 purity:0.5649988029686378 entropy:0.9046979431715131\n",
      "n_init: 100 sse:271.3675658194463 purity:0.5638017716064161 entropy:0.9054924573904617\n"
     ]
    }
   ],
   "source": [
    "for n_init in [1, 2, 3, 10, 100]:\n",
    "    km = KMeans(n_clusters=num_clusters, n_init=n_init)\n",
    "    km.fit(X)\n",
    "    centers = km.cluster_centers_\n",
    "    clusters = km.predict(X)\n",
    "    purity, entropy = eval_cluster(clusters, y, num_clusters)\n",
    "    print(f'n_init: {n_init} sse:{km.inertia_} purity:{purity} entropy:{entropy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "The most logical Expectation–Maximisation (EM) implementation within scikit-learn that is\n",
    "suitable for this data is a Gaussian Mixture model ( sklearn.mixture.GaussianMixture ; you\n",
    "can specify the number of clusters through the parameter n_components ).\n",
    "### (a) \n",
    "Contemplate what is happening in the “Expectation” step and in the “Maximisation” step\n",
    "for a Gaussian in this context.\n",
    "\n",
    "E-step: use the current estimate of the mean/standard deviation for an attribute (with respect to a cluster) to determine the (log-)likelihood of generating each instances in the training data\n",
    "\n",
    "M-stem: re-estimate each mean and standard deviation, based on the current probabilistic assignment of instances to clusters\n",
    "\n",
    "### (b) \n",
    "This version of EM uses the centroids of k-means as a seed; what effect do you think this will\n",
    "have on its behaviour?\n",
    "\n",
    "Since k-means is itself a fair clustering algorithm, it will mean that we begin with a moderately-plausible clustering - EM will consequently converge faster. On the other hand, if k-means has randomly chosen a poor clustering, then it might take a very long time to converge, or it might converge to a local minimum of the likelihood, which is possibly far worse than the global minimum.\n",
    "\n",
    "### (c) \n",
    "Compare the cluster evaluations of the EM clusters with the k-means clusters you calculated\n",
    "in the previous question. What do you observe, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster purity is 0.5702657409624132 and entropy is 0.9034605757173014\n",
      "SSE: 544.2203315337513\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=num_clusters)\n",
    "gmm.fit(X)\n",
    "centers = gmm.means_\n",
    "clusters = gmm.predict(X)\n",
    "purity, entropy = eval_cluster(clusters, y, num_clusters)\n",
    "print('cluster purity is {} and entropy is {}'.format(purity, entropy))\n",
    "sses = sse_cluster(X, clusters, centers)\n",
    "print('SSE:', np.sum(sses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences are quite small for the most part - perhaps because of the effect of using k-means to initialise the GMM.\n",
    "\n",
    "From what we can see here, it seems that the GMM might be sacrificing SSE for slightly better purity/entropy compared to KMeans. In a GMM, we can define less compact clusters (higher SSE), which in this case seems to describe the data slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
