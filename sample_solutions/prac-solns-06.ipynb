{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School of Computing and Information Systems\n",
    "\n",
    "## The University of Melbourne\n",
    "### COMP30027 MACHINE LEARNING (Semester 1, 2019)\n",
    "### Practical exercises: Week 6\n",
    "Today, we will expect you to be referring to the API for scikit-learn http://scikit-learn.org/stable/modules/classes.html — you should also refer to previous weeks’ exercises where\n",
    "necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\n",
    "We will use the Car Evaluation dataset from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data).\n",
    "\n",
    "*There is an excellent chance that you already have this data, from Project 1.*\n",
    "\n",
    "### (a) \n",
    "Load the data into a suitable format for scikit-learn , for example:\n",
    "```python\n",
    ">>> for line in f:\n",
    "        atts = line[:-1].split(\",\")\n",
    "        X.append(atts[:-1])\n",
    "        y.append(atts[-1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "with open('car.data', mode='r') as fin:\n",
    "    for line in fin:\n",
    "        atts = line.strip().split(\",\")\n",
    "        X.append(atts[:-1]) #all atts, excluding the class\n",
    "        y.append(atts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) \n",
    "How many instances are there in this collection? How many attributes, and of what type(s)?\n",
    "What is the class we’re trying to predict, and how many values does it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1728 instances\n",
      "There are 6 attributes, for example: ['vhigh', 'vhigh', '2', '2', 'small', 'low']\n",
      "There are 4 class labels: {'good', 'unacc', 'acc', 'vgood'}\n",
      "Label frequencies: [('unacc', 1210), ('acc', 384), ('good', 69), ('vgood', 65)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print('There are', len(X), 'instances')\n",
    "print('There are', len(X[0]), \"attributes, for example:\", X[0])\n",
    "print('There are', len(set(y)), \"class labels:\", set(y))   \n",
    "#use Counter to count the number of labels\n",
    "label_counter = Counter(y)\n",
    "print(\"Label frequencies: %s\" %str(label_counter.most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) \n",
    "Are there any missing attribute values? Is there any evidence that this is an artificially–\n",
    "constructed dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You might to check out the data description:*\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names\n",
    "\n",
    "*The exact (hierarchical) mechanism for determining the class value is described there. Moreover, every combination of attribute values is listed in the dataset exactly once, which is unusual for real datasets. (Effectively, we might expect that this is the entire population, and not an incomplete sample of it.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d) \n",
    "What happens if we try to build a classifier (using fit() ) using this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'vhigh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f74c72318085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m '''\n",
      "\u001b[0;32m~/ENV3/lib/python3.6/site-packages/sklearn/svm/classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr',\n\u001b[0;32m--> 227\u001b[0;31m                          dtype=np.float64, order=\"C\")\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/ENV3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'vhigh'"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X, y)\n",
    "\n",
    "'''\n",
    "The values should be numerical in scikit-learn, we need to find a way to convert string values to numbers.\n",
    "otherwise, we'd see the following errors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "Unfortunately, scikit-learn isn’t set up to deal with our attributes in this format.\n",
    "\n",
    "### (a) \n",
    "Write some functions that transform our categorical attributes into numerical attributes, by\n",
    "(perhaps arbitrarily) assigning each categorical value to an integer, for example:\n",
    "\n",
    "```python\n",
    "def convert_class(raw):\n",
    "    if raw==\"unacc\": return 0\n",
    "    elif raw==\"acc\": return 1\n",
    "    elif raw==\"good\": return 2\n",
    "    elif raw==\"vgood\": return 3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 1: {'low', 'med', 'vhigh', 'high'}\n",
      "feature 2: {'low', 'med', 'vhigh', 'high'}\n",
      "feature 3: {'5more', '2', '4', '3'}\n",
      "feature 4: {'2', '4', 'more'}\n",
      "feature 5: {'med', 'big', 'small'}\n",
      "feature 6: {'low', 'med', 'high'}\n"
     ]
    }
   ],
   "source": [
    "# We could check this from the \"car.names\" file linked above\n",
    "# Here's one (somewhat inefficient) way of reading this from the data itself\n",
    "feature_1_values = set([X[i][0] for i in range(len(X))])\n",
    "feature_2_values = set([X[i][1] for i in range(len(X))])\n",
    "feature_3_values = set([X[i][2] for i in range(len(X))])\n",
    "feature_4_values = set([X[i][3] for i in range(len(X))])\n",
    "feature_5_values = set([X[i][4] for i in range(len(X))])\n",
    "feature_6_values = set([X[i][5] for i in range(len(X))])\n",
    "print(\"feature 1: %s\" %str(feature_1_values))\n",
    "print(\"feature 2: %s\" %str(feature_2_values))\n",
    "print(\"feature 3: %s\" %str(feature_3_values))\n",
    "print(\"feature 4: %s\" %str(feature_4_values))\n",
    "print(\"feature 5: %s\" %str(feature_5_values))\n",
    "print(\"feature 6: %s\" %str(feature_6_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1728, 6), y shape: (1728,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_feature_1and2and6(raw):\n",
    "    if raw == \"low\": return 0\n",
    "    elif raw == \"med\": return 1\n",
    "    elif raw == \"high\": return 2\n",
    "    elif raw == \"vhigh\": return 3\n",
    "    # In general, we might want to catch unexpected values, too\n",
    "def convert_feature_3(raw):\n",
    "    if raw == \"2\": return 0\n",
    "    elif raw == \"3\": return 1\n",
    "    elif raw == \"4\": return 2\n",
    "    elif raw == \"5more\": return 3\n",
    "def convert_feature_4(raw):\n",
    "    if raw == \"2\": return 0\n",
    "    elif raw == \"4\": return 1\n",
    "    elif raw == \"more\": return 2\n",
    "def convert_feature_5(raw):\n",
    "    if raw == \"small\": return 0\n",
    "    elif raw == \"med\": return 1\n",
    "    elif raw == \"big\": return 2\n",
    "def convert_class(raw):\n",
    "    if raw == \"unacc\": return 0\n",
    "    elif raw == \"acc\": return 1\n",
    "    elif raw == \"good\": return 2\n",
    "    elif raw == \"vgood\": return 3\n",
    "\n",
    "X_ordinal = []\n",
    "for x in X:\n",
    "    f1, f2, f3, f4, f5, f6 = x\n",
    "    f1 = convert_feature_1and2and6(f1)\n",
    "    f2 = convert_feature_1and2and6(f2)\n",
    "    f3 = convert_feature_3(f3)\n",
    "    f4 = convert_feature_4(f4)\n",
    "    f5 = convert_feature_5(f5)\n",
    "    f6 = convert_feature_1and2and6(f6)\n",
    "    x = [f1, f2, f3, f4, f5, f6]\n",
    "    X_ordinal.append(x)\n",
    "    \n",
    "#convert to int array to make sure everything is converted.\n",
    "X_ordinal = np.array(X_ordinal, dtype='int')\n",
    "\n",
    "\n",
    "#convert ys\n",
    "y_numeric = []\n",
    "for this_y in y:\n",
    "    this_y = convert_class(this_y)\n",
    "    y_numeric.append(this_y)\n",
    "\n",
    "y_num = np.array(y_numeric, dtype='int')\n",
    "\n",
    "\n",
    "print('X shape: {}, y shape: {}'.format(X_ordinal.shape, y_num.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) \n",
    "Load the dataset again, this time as integers. Observe that we can actually build a model\n",
    "using this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_ordinal, y_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) \n",
    "Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1157, 6) X_test: (571, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # Newer versions\n",
    "#from sklearn.cross_validation import train_test_split # Older versions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33)\n",
    "print('X_train: {} X_test: {}'.format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. \n",
    "Read up on different implementations of the Naive Bayes classifier in sklearn.naive_bayes .\n",
    "Which one do you think is most suitable for the dataset we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) \n",
    "Train the (default) Naive Bayes model and determine its accuracy on the held–out test set.\n",
    "\n",
    "### (b)\n",
    "Compare the accuracies of all three different kinds of Naive Bayes classifier. Does this accord with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABCMeta', 'BaseDiscreteNB', 'BaseEstimator', 'BaseNB', 'BernoulliNB', 'ClassifierMixin', 'GaussianNB', 'LabelBinarizer', 'MultinomialNB', '_ALPHA_MIN', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_check_partial_fit_first_call', 'abstractmethod', 'binarize', 'check_X_y', 'check_array', 'check_consistent_length', 'check_is_fitted', 'issparse', 'label_binarize', 'logsumexp', 'np', 'safe_sparse_dot', 'six', 'warnings']\n",
      "GNB score 0.686515 \n",
      "MNB score 0.711033 \n",
      "BNB score 0.763573 \n",
      "GNB score 0.709282 \n",
      "MNB score 0.705779 \n",
      "BNB score 0.814361 \n",
      "GNB score 0.719790 \n",
      "MNB score 0.718039 \n",
      "BNB score 0.779335 \n",
      "Avg GNB score: 0.705195563339171\n",
      "Avg MNB score: 0.7116170461179219\n",
      "Avg BNB score: 0.7857559836544074\n"
     ]
    }
   ],
   "source": [
    "import sklearn.naive_bayes as nb\n",
    "print(dir(nb))\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "gnb_accs = []\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    gnb.fit(X_train, y_train)\n",
    "    acc = gnb.score(X_test, y_test)\n",
    "    print(\"GNB score %f \" %acc)\n",
    "    gnb_accs.append(acc)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('Avg GNB score: {}'.format(np.mean(gnb_accs)))\n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It's no real surprise that Multinomial NB doesn't work here; for example \"high\" (2), is not really \"medium\" (1) repeated twice.*\n",
    "\n",
    "*We might have expected that Gaussian NB would work a little bit here, but the ordering appears to be less significant than the feature values themselves. A secondary concern might the uniform distribution of attribute values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "By default, this implementation of Naive Bayes uses Laplace smoothing. Turn this off, and\n",
    "see what happens — what is the significance of the reported accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB score 0.711033 \n",
      "BNB score 0.763573 \n",
      "MNB score 0.705779 \n",
      "BNB score 0.814361 \n",
      "MNB score 0.718039 \n",
      "BNB score 0.779335 \n",
      "Avg MNB score: 0.7116170461179219\n",
      "Avg BNB score: 0.7857559836544074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "# Gaussian NB doesn't use smoothing; all of the probabilities for the Gaussian are already non-zero\n",
    "# You can try this for yourself, but scikit-learn will flatly refuse to do it\n",
    "#mnb = MultinomialNB(alpha=0)\n",
    "#bnb = BernoulliNB(alpha=0)\n",
    "mnb = MultinomialNB(alpha=1.0e-10)\n",
    "bnb = BernoulliNB(alpha=1.0e-10)\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Due to the implementation (as log-probabilities), numerical errors would result from unseen events.*\n",
    "\n",
    "*This is now add-k smoothing, for a very small k. You can see that the predictions are largely the same for this particular dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "What happens if you increase the smoothing parameter instead? Calculate the accuracy for\n",
    "a range of values from 5 to 500. For the very large values, examine the predicted classes for\n",
    "the test instances — what is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB score 0.698774 \n",
      "BNB score 0.698774 \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "# Let's not mess around, and go straight to a large value:\n",
    "mnb = MultinomialNB(alpha=500)\n",
    "bnb = BernoulliNB(alpha=500)\n",
    "\n",
    "for i in range(1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    print(bnb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For large values of the smoothing parameter, every instance is predicted to be the majority-class - effectively, this is the same behaviour as 0-R!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\n",
    "The transformation of the data in Q2 implicitly creates ordinal attributes. At first glance, such a\n",
    "strategy does seem reasonable in light of the given values (such as small, med, big).\n",
    "A different strategy would be to binarise the attributes: to replace a categorical attribute hav-\n",
    "ing m values with m binary attributes. One way of doing this 3 in scikit-learn is using the\n",
    "OneHotEncoder :\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X)\n",
    "X_trans = ohe.transform(X).toarray()\n",
    "```\n",
    "\n",
    "Note that this transformation should be done before we split the data into training and test sets.\n",
    "(Why?)\n",
    "\n",
    "### (a) \n",
    "Check the shape of X_trans — how many attributes do we have now? Does this correspond\n",
    "to your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1728, 21)\n",
      "feature 1: {'low', 'med', 'vhigh', 'high'}\n",
      "feature 2: {'low', 'med', 'vhigh', 'high'}\n",
      "feature 3: {'5more', '2', '4', '3'}\n",
      "feature 4: {'2', '4', 'more'}\n",
      "feature 5: {'med', 'big', 'small'}\n",
      "feature 6: {'low', 'med', 'high'}\n",
      "X[0]: ['vhigh', 'vhigh', '2', '2', 'small', 'low']\n",
      "X_trans[0]: [0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_ordinal)\n",
    "X_trans = ohe.transform(X_ordinal).toarray()\n",
    "print(X_trans.shape)\n",
    "print(\"feature 1: %s\" %str(feature_1_values))\n",
    "print(\"feature 2: %s\" %str(feature_2_values))\n",
    "print(\"feature 3: %s\" %str(feature_3_values))\n",
    "print(\"feature 4: %s\" %str(feature_4_values))\n",
    "print(\"feature 5: %s\" %str(feature_5_values))\n",
    "print(\"feature 6: %s\" %str(feature_6_values))\n",
    "print('X[0]:', X[0])\n",
    "print('X_trans[0]:', X_trans[0])\n",
    "#print(\"categories: \", ohe.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) \n",
    "\n",
    "Split the dataset comprised of one–hot attributes into train and test sets. Compare the accuracies of the three Naive Bayes models using ordinal attributes with the three models using one–hot attributes: are you surprised? What can we infer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB score 0.793345 \n",
      "MNB score 0.816112 \n",
      "BNB score 0.837128 \n",
      "GNB score 0.824869 \n",
      "MNB score 0.865149 \n",
      "BNB score 0.891419 \n",
      "GNB score 0.789842 \n",
      "MNB score 0.814361 \n",
      "BNB score 0.858144 \n",
      "Avg GNB score: 0.8026853473438411\n",
      "Avg MNB score: 0.8318739054290717\n",
      "Avg BNB score: 0.8622300058377116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "gnb_accs = []\n",
    "mnb_accs = []\n",
    "bnb_accs = []\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_trans, y_num, test_size=0.33, random_state=i)\n",
    "    gnb.fit(X_train, y_train)\n",
    "    acc = gnb.score(X_test, y_test)\n",
    "    print(\"GNB score %f \" %acc)\n",
    "    gnb_accs.append(acc)\n",
    "    \n",
    "    mnb.fit(X_train, y_train)\n",
    "    acc = mnb.score(X_test, y_test)\n",
    "    print(\"MNB score %f \" %acc)\n",
    "    mnb_accs.append(acc)\n",
    "    \n",
    "    bnb.fit(X_train, y_train)\n",
    "    acc = bnb.score(X_test, y_test)\n",
    "    print(\"BNB score %f \" %acc)\n",
    "    bnb_accs.append(acc)\n",
    "    \n",
    "print('Avg GNB score: {}'.format(np.mean(gnb_accs)))\n",
    "print('Avg MNB score: {}'.format(np.mean(mnb_accs)))\n",
    "print('Avg BNB score: {}'.format(np.mean(bnb_accs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a fairly drastic difference: Bernoulli NB is still the best option, but both Gaussian and Multinomial NB are no longer useless. It appears that all of these learners can identify meaningful patterns, just by taking the attribute value in isolation (and not in relation to the presumed ordering) - and so, perhaps our original assignment of 0,1,2,3 was too simple to discover patterns.*\n",
    "\n",
    "*At this point, we can also observe that the default behaviour of scikit-learn's Bernoulli NB is to do ... something ... with non-binary attributes, but it is usually better to make them explicitly binary using the one-hot transformer. (If you're curious, in this case, it's treating whichever value is 0 as \"N\", and the other values as \"Y\".)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. \n",
    "Recall that we built a DecisionTreeClassifier in Week 4.\n",
    "\n",
    "### (a) \n",
    "Do you think the dataset comprised of ordinal attributes, or the dataset comprised of one–hot\n",
    "attributes would be more appropriate for a typical Decision Tree? Check the test accuracy\n",
    "of the default Decision Tree model built on each of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT score (ordinal) 0.975482 \n",
      "DT score (ordinal) 0.957968 \n",
      "DT score (ordinal) 0.975482 \n",
      "Avg DT score (ordinal): 0.9696438995913602\n",
      "DT score (one-hot) 0.961471 \n",
      "DT score (one-hot) 0.947461 \n",
      "DT score (one-hot) 0.947461 \n",
      "Avg DT score (one-hot): 0.9521307647402217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=None)\n",
    "dt_accs = []\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=i)\n",
    "    \n",
    "    dt.fit(X_train, y_train)\n",
    "    acc = dt.score(X_test, y_test)\n",
    "    print(\"DT score (ordinal) %f \" %acc)\n",
    "    dt_accs.append(acc)\n",
    "print('Avg DT score (ordinal): {}'.format(np.mean(dt_accs)))\n",
    "\n",
    "dt_accs = []\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_trans, y_num, test_size=0.33, random_state=i)\n",
    "    \n",
    "    dt.fit(X_train, y_train)\n",
    "    acc = dt.score(X_test, y_test)\n",
    "    print(\"DT score (one-hot) %f \" %acc)\n",
    "    dt_accs.append(acc)\n",
    "print('Avg DT score (one-hot): {}'.format(np.mean(dt_accs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) \n",
    "How does the accuracy of the Decision Tree models compare with Naive Bayes on these\n",
    "datasets? Why might this be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since we already know that the test label is decided through a hierarchical logical process, the fact that the Decision Tree works well is probably not so surprising.*\n",
    "\n",
    "*It's a little hard to see, but the DT is slightly better on the ordinal dataset compared to the one-hot dataset.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) \n",
    "(n.b. This step might not be possible in the labs.) The main strategy for visualising a Decision\n",
    "Tree in scikit-learn is through the export_graphviz() method. Read up on the method,\n",
    "and explore whether the trees created in the previous question are indeed different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt = DecisionTreeClassifier(max_depth=None)\n",
    "dt.fit(X_ordinal, y_num)\n",
    "tree.export_graphviz(dt, out_file='tree_ordinal.dot')     \n",
    "dt.fit(X_trans, y_num)\n",
    "tree.export_graphviz(dt, out_file='tree_onehot.dot')    \n",
    "\n",
    "#print('go to http://viz-js.com/ (or any online website/tool that can visualise graphvis format) and copy/paste the content of the files to visualise the trees.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are various web-hosted toolkits for visualising the graphviz format; one example is http://vis-js.com*\n",
    "\n",
    "*You will notice that the one-hot encoded tree is larger (as it cannot group a sequence of ordered values into a single node). This does suggest that there is a little bit of significance to the attribute ordering, which a Decision Tree can learn, but Gaussian NB cannot (based on a dataset of this size).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) \n",
    "Try altering the value of the max_depth parameter, between 1 and None. Visualise the resulting trees, if you can. Compare the estimated training and test accuracies; is there any\n",
    "evidence that the algorithm is over–fitting (or under–fitting) for trees of certain depths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT ordinal depth=1: acc_train: 0.70 acc_test: 0.71\n",
      "DT ordinal depth=2: acc_train: 0.77 acc_test: 0.80\n",
      "DT ordinal depth=3: acc_train: 0.79 acc_test: 0.79\n",
      "DT ordinal depth=4: acc_train: 0.85 acc_test: 0.86\n",
      "DT ordinal depth=5: acc_train: 0.87 acc_test: 0.88\n",
      "DT ordinal depth=6: acc_train: 0.93 acc_test: 0.94\n",
      "DT ordinal depth=7: acc_train: 0.94 acc_test: 0.93\n",
      "DT ordinal depth=8: acc_train: 0.97 acc_test: 0.98\n",
      "DT ordinal depth=9: acc_train: 0.99 acc_test: 0.96\n",
      "DT ordinal depth=10: acc_train: 0.99 acc_test: 0.98\n",
      "DT ordinal depth=11: acc_train: 1.00 acc_test: 0.98\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=4)\n",
    "for max_depth in range(1, 12):\n",
    "    dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    dt.fit(X_train, y_train)\n",
    "    acc_test = dt.score(X_test, y_test)\n",
    "    acc_train = dt.score(X_train, y_train)\n",
    "    print('DT ordinal depth={}:'.format(max_depth), 'acc_train: {:.2f} acc_test: {:.2f}'.format(acc_train, acc_test))\n",
    "    tree.export_graphviz(dt, out_file='tree_ordinal_depth={}.dot'.format(max_depth))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Both the training accuracy and the test accuracy start low (for 1-R), and gradually improve. Between 8 and 10 nodes deep, the test accuracy reaches a plateau, but the training accuracy can be improved further - this is probably where over-fitting happens.*\n",
    "\n",
    "*After all, there are only 21 attribute values in total; for the deeper trees, we are considering almost all of them!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. \n",
    "The filtering approach to Feature Selection in scikit-learn can be done using SelectKBest.\n",
    "\n",
    "### (a) \n",
    "What happens to the shape of X_train and X_test now?\n",
    "\n",
    "### (b)\n",
    "Find out what the best features were for your dataset, according to $\\chi^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1157, 3)\n",
      "(571, 3)\n",
      "0\n",
      "3\n",
      "5\n",
      "(1157, 5)\n",
      "(571, 5)\n",
      "0\n",
      "12\n",
      "13\n",
      "18\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_num, test_size=0.33, random_state=4)\n",
    "\n",
    "x2 = SelectKBest(chi2, k=3)\n",
    "X_train = x2.fit_transform(X_train,y_train)\n",
    "X_test = x2.transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(feat_num)\n",
    "    \n",
    "#It's more difficult to keep track of the various attributes after they have been one-hot encoded:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_trans, y_num, test_size=0.33, random_state=4)\n",
    "\n",
    "x2 = SelectKBest(chi2, k=5)\n",
    "X_train = x2.fit_transform(X_train,y_train)\n",
    "X_test = x2.transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "for feat_num in x2.get_support(indices=True):\n",
    "    print(feat_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comparing these, it's striking that the best attributes overall are quite different to the best unique attribute values. Of course, we would rather not do feature selection over the one-hot data, because there will be numerous test instances that don't have any of the most predictive values!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
