{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from random import randint, seed\n",
    "seed(30027)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "570"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = ['alt.atheism','talk.religion.misc']\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=30027)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=30027)\n",
    "\n",
    "X_train = data_train.data\n",
    "y_train = data_train.target\n",
    "\n",
    "X_test = data_test.data\n",
    "y_test = data_test.target\n",
    "\n",
    "len(X_train)\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: acooper@mac.cc.macalstr.edu\n",
      "Subject: Re: Where are they now?\n",
      "Organization: Macalester College\n",
      "Lines: 38\n",
      "\n",
      "In article <1qi156INNf9n@senator-bedfellow.MIT.EDU>, tcbruno@athena.mit.edu (Tom Bruno) writes:\n",
      "> \n",
      "> Wow.  Leave your terminal for a few months and everyone you remember goes\n",
      "> away-- how depressing.  Actually, there are a few familiar faces out there,\n",
      "> counting Bob and Kent, but I don't seem to recognize anyone else.  Has anyone\n",
      "> heard from Graham Matthews recently, or has he gotten his degree and sailed\n",
      "> for Greener Pastures (tm)?  \n",
      "> \n",
      "> Which brings me to the point of my posting.  How many people out there have \n",
      "> been around alt.atheism since 1990?  I've done my damnedest to stay on top of\n",
      "> the newsgroup, but when you fall behind, you REALLY fall behind (it's still not\n",
      "> as bad as rec.arts.startrek used to be, but I digress).  Has anyone tried to\n",
      "> keep up with the deluge?  Inquiring minds want to know!  Also-- does anyone\n",
      "> keep track of where the more infamous posters to alt.atheism end up, once they\n",
      "> leave the newsgroup?  Just curious, I guess.\n",
      "> \n",
      "> cheers,\n",
      "> tom bruno\n",
      "\n",
      "\n",
      "I am one of those people who always willl have unlimited stores of unfounded\n",
      "respect for people who have been on newsgroups/mailing lists longer than I\n",
      "have, so you certainly have my sympathy Tom.  I have only been semi-regularly\n",
      "posting (it is TOUGHto keep up) since this February, but I have been reading\n",
      "and following the threads since last August: my school's newsreader was down\n",
      "for months and our incompetent computing services never bothered to find a new\n",
      "feed site, so it wasn't accepting outgoing postings.  I don't think anyone\n",
      "keeps track of where other posters go: it's that old love 'em and leave 'em\n",
      "Internet for you again...\n",
      "\n",
      "\n",
      "best regards,\n",
      "\n",
      "********************************************************************************\n",
      "* Adam John Cooper\t\t\"Verily, often have I laughed at the weaklings *\n",
      "*\t\t\t\t   who thought themselves good simply because  *\n",
      "* acooper@macalstr.edu\t\t\t\tthey had no claws.\"\t       *\n",
      "********************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above text is from \"alt.atheism\" because they mention \"alt.atheism\" several times in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the guess:\n",
    "y_train[0]\n",
    "# true!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Question 1 (c) - can we train on X directly?\"\"\"\n",
    "Can't train on the text directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question 2 (b) - CountVectorizer'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Question 2 (b) - CountVectorizer\"\"\"\n",
    "\n",
    "vectoriser = CountVectorizer()\n",
    "X_train_vec = vectoriser.fit_transform(X_train)\n",
    "X_test_vec = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What transformation does count vectorizer apply to the data?\n",
    "\n",
    "it transforms each raw document to a vector of \"token counts\"\n",
    "\n",
    "each column pertains to one of the words in the vocabulary of the whole corpus\n",
    "\n",
    "each cell value is a count of the number of times that word has appeared in that document\n",
    "\n",
    "Weakness of this kind of preprocessing for textual data?\n",
    "\n",
    "very wide instances (sparse), which require feature selection\n",
    "\n",
    "lose context - the order in which words occur and the proximity of words to each other, e.g. \"the baby bites the dog\" vs \"the dog bites the baby\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 18089)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(570, 18089)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec.shape\n",
    "X_test_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all columns contain at least one non-zero entry\n",
    "for j in range(570):\n",
    "    all_zero = True\n",
    "    for i in range(18089):\n",
    "        if X_test_vec[j, i] != 0:\n",
    "            all_zero = False\n",
    "            break\n",
    "    if all_zero == True:\n",
    "        print(j)\n",
    "\n",
    "# but in reality, it's not impossible, e.g. all words in a test document have not be seen in training document (e.g. empty document or different languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question 2 (a) - DictVectorizer'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 1., 0.],\n",
       "       [3., 0., 0., 1.],\n",
       "       [0., 6., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Question 2 (a) - DictVectorizer\"\"\"\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "example_data = [\n",
    "    {\"a\":2,        \"c\":\"hello\"}, # no data for \"b\"\n",
    "    {\"a\":3, \"b\":0, \"c\":\"world\"},\n",
    "    {\"a\":0, \"b\":6             }  # no data for \"c\"\n",
    "]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "example_transformed = vectorizer.fit_transform(example_data)\n",
    "example_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(224.60860862640754, 'atheists'),\n",
       " (220.94682964410734, 'keith'),\n",
       " (218.65880101721638, 'atheism'),\n",
       " (160.67589129636838, 'caltech'),\n",
       " (132.57334250013005, 'christ'),\n",
       " (117.58179369988414, 'ra'),\n",
       " (111.25019141016739, 'jesus'),\n",
       " (101.74299973783235, 'islamic'),\n",
       " (99.93808580485398, 'brian'),\n",
       " (99.8538942196429, 'atheist')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(857, 10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(0.09558402391043085, 'atheists'),\n",
       " (0.07532480782080347, 'keith'),\n",
       " (0.07114535053559949, 'the'),\n",
       " (0.0706314729031248, 'cco'),\n",
       " (0.06791361176738653, 'caltech'),\n",
       " (0.06568123450982752, 'schneider'),\n",
       " (0.06332024200377574, 'allan'),\n",
       " (0.06204259837971008, 'of'),\n",
       " (0.0447927508127979, 'atheism'),\n",
       " (0.04473706629144185, 'it')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "for method in [chi2, mutual_info_classif]:\n",
    "    \n",
    "    selector = SelectKBest(method, k=10)\n",
    "\n",
    "    X_train1 = selector.fit_transform(X_train_vec, y_train)\n",
    "\n",
    "    X_train1.shape\n",
    "\n",
    "    # fetch the scores for the K best features\n",
    "    scores = selector.scores_[selector.get_support(indices=True)]\n",
    "\n",
    "    # fetch the names (tokens) for the K best features\n",
    "    names = [vectoriser.get_feature_names()[i] for i in selector.get_support(indices=True)]\n",
    "\n",
    "    # sort by score, print out\n",
    "    scores_names = sorted(zip(scores, names), reverse=True)\n",
    "    display(scores_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the tokens seem relevant to distinguishing between document types?\n",
    "\n",
    "Yes, broadly relevant e.g. \"atheists\", \"atheism\", \"jesus\", \"islamic\"\n",
    "\n",
    "There are some weird features e.g. \"brian\", \"ra\", \"keith\" (peculiarity of dataset, small size of the dataset)\n",
    "\n",
    "IS there evidence of the relative biases of chi_2 and PMI\n",
    "chi^2 is alleged to prefer \"uncommon\" words\n",
    "PMI is allegd to (relative to chi^2) prefer \"common\" words\n",
    "\n",
    "The bias is nto particularly noticeable for the chi^2 features\n",
    "The bias for PMi is noticable, more common words like \"of\", \"it\", \"the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2536,  2537,  2540,  3345,  3634,  4029,  9309,  9441,  9644,\n",
       "       13470])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.feature_selection.mutual_info_.mutual_info_classif>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
